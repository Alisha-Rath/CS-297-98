<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
        "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <title>Chris Pollett &gt; Students &gt; Alisha Rath</title>
    <meta http-equiv="Content-type" content="text/html; charset=UTF-8" />

    <meta name="Authors" content="Christopher Pollett" />
    <meta name="keywords" content="Alisha Rath" />
    <style type="text/css">
    <!--
        
.indent {margin-left: 3%}
.floatRight {float:right;}
.nobot {line-height: .05in; margin-bottom: .07in}
.contentColor {background: #FFFFFF; color:#000000}
.borderColor {background: #99FFCC; color:#000000}
.borderColor2 {background: #99FF99; color:#000000}
.grayBackground {background: #EEEEEE;}
.blackBackground {background:black; color:white;}
.centerTable {margin-left:20%; margin-right:20%;}
.italic {font-style: italic}
.bold {font-weight: bold}
.underline {text-decoration: underline}
.tt {font-family: Courier}
.center {text-align:center}
.red {color:#FF0000}
.white {color:#FFFFFF}
.hideme {display: none}

.whitebox {
background-color: white;
border: solid;
border-color: black;
border-width: 1px;
padding: 0.25em;
}

.beigebox {
background-color: #EEEEEE;
border: solid;
border-color: black;
border-width: 2px;
padding: 0.25em;
}

.h1 {font-size: 16pt; font-weight:bold}
.h2 {font-size: 14pt; font-weight:bold}
.h3 {font-size: 12pt; font-weight:bold}
.h4 {font-size: 10pt; font-weight:bold}

.right {text-align:right}
.left {text-align:left}

.borderTable, .borderTable th, .borderTable td
{
    border: 1px solid black;
}
    -->
    </style>
</head>

<body class="contentColor" style="margin-left:0cm;
margin-right: 0cm; margin-top:0cm; margin-bottom:0cm">

<table border="0" cellpadding="0" cellspacing="0" style="height:10in;">

        <col width="25%"/>
    
    <col width="75%" />
    <tr>
        <td class="borderColor"   valign="top">
    <p><b><a href="http://www.cs.sjsu.edu/faculty/pollett/" >Chris Pollett</a> &gt;
    <a href="http://www.cs.sjsu.edu/faculty/pollett/masters/" >Students</a> &gt;<br />
    Alisha</b><br />
&#160;&#160;&#160;&#160;( <a href="?Deliverable_1.html?print">Print View</a>)
            </p>
        <p>&#160;&#160;&#160;&#160;[<a href="?Bio.php">Bio</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?Blog.php">Blog</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?CS297Proposal.html">C297 Proposal</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="LLM_Overview_and_Usage.pdf">LLM Overview and Usage - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="LoRA (Low-Rank Adaptation) for-Supervised Learning.pdf">LoRA Supervised Learning - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="Machine_Court_Justice_Presentation.pdf">Paper_Using_DataSet - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="DistilBERT_FineTuning_LegalCasePrediction.pdf">LLM Base Model1: DistilBERT - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="Different_fine_tuning_models.pdf">Different Fine-Tuning Models - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="Explainable_AI_and_LORA.pdf">Explainable AI With LORA - PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="TaskTypes in Peft.pdf">TaskTypes In PEFT- PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?Deliverable_1.html">Deliverable_1</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?Deliverable_2.html">Deliverable_2</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?Deliverable_3.html">Deliverable_3</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?Deliverable_4.html">Deliverable_4</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="CS297_report.pdf">CS297-report- PDF</a>]</p>
    <p>&#160;&#160;&#160;&#160;[<a href="?CS298_proposal.html">CS298 Proposal</a>]</p>
        </td>
                <td class="contentColor" valign="top">
            <a id="top" />
                        <div class="center">
<h1>Fine-Tuning Insight</h1>
</div>
<h2><p><b>Description:</b></p></h2>
<p>This deliverable focuses on learning and understanding supervised fine-tuning using Large Language Models (LLMs) for AI models, particularly in the context of predicting legal case outcomes. It involved following a detailed tutorial, studying the concepts of supervised learning, and experimenting with a parameter-efficient tuning method called LoRA.</p>

<h2><p><b>Steps Followed:</b></p></h2>
<p>Here is a detailed breakdown of the steps I took during this phase:</p>

<h3>Step 1: Understanding Supervised Learning</h3>
<p>To build a foundation, I explored resources on supervised learning, including articles and instructional videos. I focused on the following aspects:</p>
<ul>
    <li><strong>Supervised Learning Basics:</strong> Learning how models map input data to labeled outputs using datasets.</li>
    <li><strong>Applications:</strong> Examples include classification tasks like sentiment analysis and legal decision-making.</li>
    <li><strong>Challenges:</strong> Understanding the need for large labeled datasets and their impact on training accuracy.</li>
</ul>

<h3>Step 2: Exploring Fine-Tuning Concepts</h3>
<p>I delved into fine-tuning, which involves refining a pre-trained model for a specific task. Key insights include:</p>
<ul>
    <li><strong>Transfer Learning:</strong> Utilizing a pre-trained model and adjusting it with a task-specific dataset.</li>
    <li><strong>LoRA Technique:</strong> Parameter-efficient fine-tuning, freezing most layers while training small additional parameters to improve model adaptation.</li>
</ul>

<h3>Step 3: Hands-On Tutorial - Fine-Tuning with LoRA</h3>
<p>I followed a tutorial for fine-tuning the DistilBERT model using LoRA, covering the following steps:</p>
<ul>
    <li><strong>Environment Setup:</strong> Set up the Python environment using Google Colab and installed required libraries like Hugging Face Transformers and LoRA tools.</li>
    <li><strong>Model Loading:</strong> Loaded the DistilBERT pre-trained model from Hugging Face.</li>
    <li><strong>Data Preparation:</strong> Cleaned and tokenized a sentiment analysis dataset.</li>
    <li><strong>Fine-Tuning:</strong> Configured LoRA to adjust only a subset of model parameters and trained over multiple epochs.</li>
    <li><strong>Evaluation:</strong> Analyzed model accuracy and fine-tuning impact using a test dataset.</li>
</ul>

<h2>Code Snippet</h2>
<p>1. Below is an image showing the code snippet used for fine-tuning using LoRA:</p>
<img src="train_model.png" alt="Code snippet for fine-tuning distilbert model using LoRA" style="width:50%;">

<p>2. Below is an image showing the code snippet of the accuracy of the model after fine-tuning on the dataset:</p>
<img src="accuracy_per_epoch.png" alt="Code snippet of the accuracy of the model after fine-tuning on the dataset" style="width:50%;">

<p>3. Below is an image showing the code snippet of the result of the model before training:</p>
<img src="untrained_model.png" alt="Code snippet of the result of the model before training" style="width:50%;">

<p>4. Below is an image showing the code snippet of the result of the model after training:</p>
<img src="trained_model.png" alt="Code snippet of the result of the model after training" style="width:50%;">

<h2><p><b>Insights Gained:</b></p></h2>
<p>This tutorial helped me understand how fine-tuning enhances model accuracy for domain-specific tasks. It also highlighted the advantages of using LoRA for parameter-efficient tuning, making it possible to achieve good results with limited computational resources.</p>

<h2><p><b>Next Steps:</b></p></h2>
<p>Building on this knowledge, I plan to apply these fine-tuning techniques to my legal decision case prediction model, adjusting parameters based on the characteristics of the legal data and optimizing the model for better predictions.</p>

<h2><p><b>Tutorial Reference:</b></h2> You can find the tutorial I followed for this process <a href="https://www.youtube.com/watch?v=eC6Hd1hFvos&t=545s">Fine-tuning Large Language Models (LLMs) | w/ Example Code</a>.</p>

<h2><p><b>Code Reference:</b></p></h2>
<p>You can download the complete Jupyter notebook used for this tutorial by clicking the link below:</p>
<a href="Hugging_Face_Transformers_supervised_fine_tuning_LoRA.ipynb" download>Hugging_Face_Transformers_supervised_fine_tuning_LoRA (.ipynb)</a>

<h2><p><b>Presentation links:</b></p></h2>
<p>You can see the PDF on the left side of the page too:</p>
<p><a href="LLM_Overview_and_Usage.pdf">LLM Overview and Usage - PDF</a> </p>
<p><a href="LoRA (Low-Rank Adaptation) for-Supervised Learning.pdf">LoRA (Low-Rank Adaptation) for Supervised Learning - PDF</a> </p>
                    </td>
    </tr>

</table>
</body>

</html>

